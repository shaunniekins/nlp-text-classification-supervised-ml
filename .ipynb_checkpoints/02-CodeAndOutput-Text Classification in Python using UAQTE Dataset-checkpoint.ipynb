{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1155e8a6",
   "metadata": {
    "id": "1155e8a6"
   },
   "source": [
    "<h1>Text Classification using Python (e-Participation 2.1)</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b8839",
   "metadata": {
    "id": "8b4b8839"
   },
   "source": [
    "<h1><b>INSTALLING PACKAGES</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6373bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5502,
     "status": "ok",
     "timestamp": 1697698027329,
     "user": {
      "displayName": "Kosaki Onodera",
      "userId": "04711233799438479357"
     },
     "user_tz": -480
    },
    "id": "5e6373bf",
    "outputId": "c9f185fc-5c54-4b37-d44d-f86457b234df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902f7fcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6466,
     "status": "ok",
     "timestamp": 1697698033794,
     "user": {
      "displayName": "Kosaki Onodera",
      "userId": "04711233799438479357"
     },
     "user_tz": -480
    },
    "id": "902f7fcb",
    "outputId": "fbfd2a57-77ad-457a-8cbb-14d2a07e7a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84126a5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1697698035400,
     "user": {
      "displayName": "Kosaki Onodera",
      "userId": "04711233799438479357"
     },
     "user_tz": -480
    },
    "id": "84126a5b",
    "outputId": "5b11e13f-12dd-43be-ddcd-4e4ed4ab852c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post10.tar.gz (3.6 kB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b9a11d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7410,
     "status": "ok",
     "timestamp": 1697698042803,
     "user": {
      "displayName": "Kosaki Onodera",
      "userId": "04711233799438479357"
     },
     "user_tz": -480
    },
    "id": "d5b9a11d",
    "outputId": "81e44162-7635-4418-92f0-905bf5f2f012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61388e47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5813,
     "status": "ok",
     "timestamp": 1697698048608,
     "user": {
      "displayName": "Kosaki Onodera",
      "userId": "04711233799438479357"
     },
     "user_tz": -480
    },
    "id": "61388e47",
    "outputId": "1c5c07bb-5d9e-4fdd-e862-a398a93e7a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4218d",
   "metadata": {
    "id": "afa4218d"
   },
   "source": [
    "<h1><b>IMPORTING LIBRARIES</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d50bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5497,
     "status": "ok",
     "timestamp": 1697693546166,
     "user": {
      "displayName": "Vaughn Ola-o",
      "userId": "08515038653180739714"
     },
     "user_tz": -480
    },
    "id": "6e5d50bb",
    "outputId": "11594657-323e-463d-e947-57184339b499"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#CELL No. 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt') #divides a text into list of sentences\n",
    "nltk.download('averaged_perceptron_tagger') #POS tagger\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#performance metrics\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "#from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebc00b",
   "metadata": {
    "id": "64ebc00b"
   },
   "source": [
    "<h1><b>LOADING AND EXPLORING THE DATASET</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb0b94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1697693546166,
     "user": {
      "displayName": "Vaughn Ola-o",
      "userId": "08515038653180739714"
     },
     "user_tz": -480
    },
    "id": "a3bb0b94",
    "outputId": "5161cd5c-ace9-4d57-9b41-50253a3f890a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-76c9a33b18d9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import social media dataset and load to a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_uaqte\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uaqte_balanced_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_uaqte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_uaqte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'uaqte_balanced_dataset.csv'"
     ]
    }
   ],
   "source": [
    "#CELL NO. 2\n",
    "\n",
    "# Import social media dataset and load to a dataframe\n",
    "\n",
    "df_uaqte=pd.read_csv('uaqte_balanced_dataset.csv')\n",
    "print(df_uaqte.shape)\n",
    "df_uaqte.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faffb6b",
   "metadata": {
    "id": "5faffb6b"
   },
   "outputs": [],
   "source": [
    "#CELL NO. 3\n",
    "# CLASS DISTRIBUTION – check if dataset is balanced or not\n",
    "\n",
    "# Labels:\n",
    "# 0 - negative\n",
    "# 1 - positive\n",
    "# 2 - neutral\n",
    "\n",
    "x=df_uaqte['label'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x=x.index, y=x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77e5db",
   "metadata": {
    "id": "4a77e5db"
   },
   "outputs": [],
   "source": [
    "#CELL NO. 4\n",
    "#WORD-COUNT\n",
    "print('Word Count:')\n",
    "df_uaqte['word_count'] = df_uaqte['text'].apply(lambda x: len(str(x).split()))\n",
    "print('\\tPositive Comment/Text: ', df_uaqte[df_uaqte['label']==1]['word_count'].mean()) #Positive\n",
    "print('\\tNegative Comment/Text: ', df_uaqte[df_uaqte['label']==0]['word_count'].mean()) #Negative\n",
    "print('\\tNeutral Comment/Text: ', df_uaqte[df_uaqte['label']==2]['word_count'].mean()) #Neutral\n",
    "\n",
    "#2. CHARACTER-COUNT\n",
    "print('\\nCharacter Count:')\n",
    "df_uaqte['char_count'] = df_uaqte['text'].apply(lambda x: len(str(x)))\n",
    "print('\\tPositive Comment/Text: ', df_uaqte[df_uaqte['label']==1]['char_count'].mean()) #Positive\n",
    "print('\\tNegative Comment/Text: ', df_uaqte[df_uaqte['label']==0]['char_count'].mean()) #Negative\n",
    "print('\\tNeutral Comment/Text: ', df_uaqte[df_uaqte['label']==2]['char_count'].mean()) #Neutral\n",
    "\n",
    "\n",
    "#3. UNIQUE WORD-COUNT\n",
    "print('\\nUnique Word Count:')\n",
    "df_uaqte['unique_word_count'] = df_uaqte['text'].apply(lambda x: len(set(str(x).split())))\n",
    "print('\\tPositive Comment/Text: ', df_uaqte[df_uaqte['label']==1]['unique_word_count'].mean()) #Positive\n",
    "print('\\tNegative Comment/Text: ', df_uaqte[df_uaqte['label']==0]['unique_word_count'].mean()) #Negative\n",
    "print('\\tNeutral Comment/Text: ', df_uaqte[df_uaqte['label']==2]['unique_word_count'].mean()) #Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819804c",
   "metadata": {
    "id": "7819804c"
   },
   "outputs": [],
   "source": [
    "#CELL NO. 5\n",
    "#Plotting word-count per label/category\n",
    "\n",
    "#plot for positive sentiments\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=df_uaqte[df_uaqte['label']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Negative')\n",
    "\n",
    "#plot for negative sentiments\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=df_uaqte[df_uaqte['label']==0]['word_count']\n",
    "ax1.hist(train_words,color='blue')\n",
    "ax1.set_title('Negative')\n",
    "\n",
    "#plot for neutral sentiments\n",
    "train_words=df_uaqte[df_uaqte['label']==2]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Neutral')\n",
    "fig.suptitle('Words per text')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a0eba",
   "metadata": {
    "id": "a95a0eba"
   },
   "source": [
    "<h1><b>PRE-PROCESSING</b></h1>\n",
    "<br>\n",
    "</t>Next cell demonstrates how to preprocess the dataset by removing punctuations & special characters, cleaning texts, removing stop words, and applying lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a560e72",
   "metadata": {
    "id": "3a560e72"
   },
   "source": [
    "<h5>1. Simple Text Cleaning</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4f46a",
   "metadata": {
    "id": "40d4f46a"
   },
   "outputs": [],
   "source": [
    "#CELL NO. 6\n",
    "\n",
    "#1. Common text preprocessing\n",
    "text = \"   This is a message to be cleaned. It may involve some things like: , ?, :, ''  adjacent spaces and tabs     .  \"\n",
    "\n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace\n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace\n",
    "\n",
    "    return text\n",
    "\n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab16675",
   "metadata": {
    "id": "2ab16675"
   },
   "outputs": [],
   "source": [
    "#CELL NO.7\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "# Get the list of stopwords for a specific language (e.g., English)\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Print the list of stopwords\n",
    "print(stopwords_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc15328",
   "metadata": {
    "id": "2cc15328"
   },
   "outputs": [],
   "source": [
    "#CELL NO.8\n",
    "\n",
    "# Define a list of common Tagalog stopwords\n",
    "tagalog_stopwords = [\n",
    "    'ako', 'alin', 'am', 'amin', 'aming', 'ang', 'ano', 'anumang', 'apat', 'at',\n",
    "    'atin', 'ating', 'ay', 'bababa', 'bago', 'bakit', 'bawat', 'bilang', 'dahil',\n",
    "    'dalawa', 'dapat', 'din', 'dito', 'doon', 'gagawin', 'gayunman', 'ginagawa',\n",
    "    'ginawa', 'ginawang', 'gumawa', 'gusto', 'habang', 'hanggang', 'hindi', 'huwag',\n",
    "    'iba', 'ibaba', 'ibabaw', 'ibig', 'ikaw', 'ilagay', 'ilalim', 'ilan', 'inyong',\n",
    "    'isa', 'isang', 'itaas', 'ito', 'iyo', 'iyon', 'iyong', 'ka', 'kahit', 'kailangan',\n",
    "    'kailanman', 'kami', 'kanila', 'kanilang', 'kanino', 'kanya', 'kanyang', 'kapag',\n",
    "    'kapwa', 'karamihan', 'katiyakan', 'katulad', 'kaya', 'kaysa', 'ko', 'kong', 'kulang',\n",
    "    'kumuha', 'kung', 'laban', 'lahat', 'lamang', 'likod', 'lima', 'maaari', 'maaaring',\n",
    "    'maging', 'mahusay', 'makita', 'marami', 'marapat', 'mga', 'minsan', 'mismo', 'mula',\n",
    "    'muli', 'na', 'nabanggit', 'naging', 'nagkaroon', 'nais', 'nakita', 'namin', 'napaka',\n",
    "    'narito', 'nasaan', 'ng', 'nga', 'ngayon', 'ni', 'nila', 'nilang', 'nito', 'niya',\n",
    "    'niyang', 'noon', 'o', 'pa', 'paano', 'pababa', 'paggawa', 'pagitan', 'pagkakaroon',\n",
    "    'pagkatapos', 'palabas', 'pamamagitan', 'panahon', 'pangalawa', 'para', 'paraan',\n",
    "    'pareho', 'pataas', 'pero', 'pumunta', 'pumupunta', 'sa', 'saan', 'sabi', 'sabihin',\n",
    "    'sarili', 'sila', 'sino', 'siya', 'tatlo', 'tayo', 'tulad', 'tungkol', 'una', 'walang',\n",
    "    'ito', 'iyan'\n",
    "]\n",
    "\n",
    "# Print the list of Tagalog stopwords\n",
    "print(tagalog_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25616588",
   "metadata": {
    "id": "25616588"
   },
   "source": [
    "<h5>2. Lexicon-based Text Preprocessing</h5><br>\n",
    " a. Stopword removal - removing insignificant words from English vocabulary using nltk. A few such words are ‘i’,’you’,’a’,’the’,’he’,’which’ etc.\n",
    "<br> b. Stemming - process of slicing the end or the beginning of words with the intention of removing affixes(prefix/suffix)\n",
    "<br> c. Lemmatization - process of reducing the word to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36e6f8",
   "metadata": {
    "id": "fb36e6f8"
   },
   "outputs": [],
   "source": [
    "#CELL NO.9\n",
    "# LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
    "\n",
    "#1. STOP WORDS REMOVAL\n",
    "def stopword(string):\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    combined_stopwords = english_stopwords + tagalog_stopwords\n",
    "\n",
    "    words = [word for word in string.split() if word.lower() not in combined_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "text=stopword(text)\n",
    "print(text)\n",
    "\n",
    "#2. STEMMING\n",
    "\n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "text=stemming(text)\n",
    "print(text)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "text = lemmatizer(text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c06c9",
   "metadata": {
    "id": "5d2c06c9"
   },
   "source": [
    "<h5>Final Preprocessing on our Dataset</h5><br>\n",
    "Applying all the preprocessing functions defined above to the data frame (df_uaqte / uaqte_balanced_dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8a98e",
   "metadata": {
    "id": "30f8a98e"
   },
   "outputs": [],
   "source": [
    "#CELL NO.10\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_uaqte['clean_text'] = df_uaqte['text'].apply(lambda x: finalpreprocess(x))\n",
    "\n",
    "df_uaqte.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59a011",
   "metadata": {
    "id": "6c59a011"
   },
   "source": [
    "<h1>FEATURE EXTRACTION</h1>\n",
    "Extracting vectors from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff9a18",
   "metadata": {
    "id": "67ff9a18"
   },
   "source": [
    "<b>Splitting the dataset using 80:20 ratio. 80% as training set and 20% as test set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da10bd",
   "metadata": {
    "id": "a6da10bd"
   },
   "outputs": [],
   "source": [
    "#CELL NO.11\n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_uaqte[\"clean_text\"],\n",
    "                                                  df_uaqte[\"label\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "\n",
    "print(\"DONE SPLITTING AND WORK TOKENIZING.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c650395",
   "metadata": {
    "id": "1c650395"
   },
   "source": [
    "<b>Extracting features/ vectors using Bag-of-words(with Tf-\n",
    "   Idf) and Word2Vec</b>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a35fa",
   "metadata": {
    "id": "c70a35fa"
   },
   "outputs": [],
   "source": [
    "#CELL NO.12\n",
    "# create Word2vec model\n",
    "\n",
    "df_uaqte['clean_text_tok']=[nltk.word_tokenize(i) for i in df_uaqte['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(df_uaqte['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "print(\"DONE RUNNING.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac81d28",
   "metadata": {
    "id": "cac81d28"
   },
   "outputs": [],
   "source": [
    "#CELL NO. 13\n",
    "\n",
    "#TF-IDF\n",
    "# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
    "\n",
    "# Only transform x_test (not fit and transform)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will\n",
    "\n",
    "#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
    "#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without\n",
    "#it, and the have compatible\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)\n",
    "\n",
    "print(\"DONE CREATING VECTORS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a947892",
   "metadata": {
    "id": "0a947892"
   },
   "source": [
    "<h1>TRAINING MODELS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a157a",
   "metadata": {
    "id": "248a157a"
   },
   "source": [
    "<h3>Multinomial Logistic Regression with TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50fd6da",
   "metadata": {
    "id": "f50fd6da"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#CELL NO.14\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lr_tfidf\u001b[38;5;241m=\u001b[39mLogisticRegression(solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultinomial\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      4\u001b[0m lr_tfidf\u001b[38;5;241m.\u001b[39mfit(X_train_vectors_tfidf, y_train)  \u001b[38;5;66;03m#model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Predict y value for test dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "#CELL NO.14\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
    "\n",
    "#Generate confusion matrix\n",
    "conf_matrix = confusion_matrix (y_val, y_predict)\n",
    "\n",
    "#Print accuracy score and classification report\n",
    "print('Accuracy: %s\\n' % metrics.accuracy_score(y_predict, y_val))\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix: \\n',conf_matrix)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=lr_tfidf.classes_, yticklabels=lr_tfidf.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76680d36",
   "metadata": {
    "id": "76680d36"
   },
   "source": [
    "<h3>Naive Bayes with TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3038e90",
   "metadata": {
    "id": "c3038e90"
   },
   "outputs": [],
   "source": [
    "#CELL NO.14\n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
    "\n",
    "#Generate confusion matrix\n",
    "conf_matrix = confusion_matrix (y_val, y_predict)\n",
    "\n",
    "#Print accuracy score and classification report\n",
    "print('Accuracy: %s\\n' % metrics.accuracy_score(y_predict, y_val))\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix: \\n',conf_matrix)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=nb_tfidf.classes_, yticklabels=nb_tfidf.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87313f5b",
   "metadata": {
    "id": "87313f5b"
   },
   "source": [
    "<h3>Multinomial Logistic Regression with Word2Vec</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711a9a4",
   "metadata": {
    "id": "8711a9a4"
   },
   "outputs": [],
   "source": [
    "#CELL NO.15\n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "\n",
    "#Generate confusion matrix\n",
    "conf_matrix = confusion_matrix (y_val, y_predict)\n",
    "\n",
    "#Print accuracy score and classification report\n",
    "print('Accuracy: %s\\n' % metrics.accuracy_score(y_predict, y_val))\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix: \\n',conf_matrix)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=lr_w2v.classes_, yticklabels=lr_w2v.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2c32f",
   "metadata": {
    "id": "2ca2c32f"
   },
   "source": [
    "<h3>Linear SVM with Word2Vec</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b566c11",
   "metadata": {
    "id": "5b566c11"
   },
   "outputs": [],
   "source": [
    "#CELL NO.16\n",
    "#FITTING THE CLASSIFICATION MODEL using Linear SVM (W2v)\n",
    "svm_w2v=sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=123, max_iter=5, tol=None)\n",
    "\n",
    "svm_w2v.fit(X_train_vectors_w2v, y_train)#model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = svm_w2v.predict(X_val_vectors_w2v)\n",
    "\n",
    "#Generate confusion matrix\n",
    "conf_matrix = confusion_matrix (y_val, y_predict)\n",
    "\n",
    "#Print accuracy score and classification report\n",
    "print('Accuracy: %s\\n' % metrics.accuracy_score(y_predict, y_val))\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix: \\n',conf_matrix)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=svm_w2v.classes_, yticklabels=svm_w2v.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3ce60",
   "metadata": {
    "id": "a3d3ce60"
   },
   "source": [
    "<h1>GENERATE PREDICTIONS USING THE BEST CLASSIFIER MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a5ca8",
   "metadata": {
    "id": "598a5ca8"
   },
   "outputs": [],
   "source": [
    "#CELL NO.17\n",
    "\n",
    "#Testing it on new dataset with the best model\n",
    "df_test=pd.read_csv('make_predictions.csv')  #reading the data\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=df_test['clean_text']\n",
    "\n",
    "X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
    "y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['label']= y_predict\n",
    "\n",
    "print(df_test.head())\n",
    "final=df_test[['text','label']].reset_index(drop=True)\n",
    "final.to_csv('submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f9fa4",
   "metadata": {
    "id": "810f9fa4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
